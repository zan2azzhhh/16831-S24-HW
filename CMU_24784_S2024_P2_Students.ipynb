{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zan2azzhhh/16831-S24-HW/blob/main/CMU_24784_S2024_P2_Students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eeje4O8fviH",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 24784 - Trustworthy AI Autonomy (Homework 2)\n",
        "\n",
        "## Instructions\n",
        "In this homework you are going to experiment with two model-based reinforcement learning (MBRL) algorithms. You will first code the missing part in models using [PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) and [GPyTorch](https://docs.gpytorch.ai/en/stable/).\n",
        "You will then implement your algorithms to control a vehicle in an parking in [highway-env](https://github.com/eleurent/highway-env). This homework is modefied based on the colab [template](https://colab.research.google.com/github/eleurent/highway-env/blob/master/scripts/parking_model_based.ipynb#scrollTo=NwCDLD1wfvi2) from Edouard Leurent.\n",
        "\n",
        "The main experiments uses **parking-v0** environment. The goal is to park a vehicle to a given goal location with the appropriate heading by controlling the gas pedal and steering angle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S06jxQW11jjX"
      },
      "source": [
        "## 0. Install and import packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I-F0X0LuwlB"
      },
      "source": [
        "# Install environment\n",
        "!pip install numpy==1.23.5\n",
        "!pip install highway-env==1.5\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get update\n",
        "\n",
        "# Install visualization and Gaussian processes dependencies\n",
        "!apt-get install -y xvfb python3-opengl ffmpeg -y\n",
        "!pip install gpytorch\n",
        "!pip install moviepy\n",
        "!pip install imageio_ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMSuJEOfviP",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "# Import environment\n",
        "import gym\n",
        "import highway_env\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Models and computation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "import gpytorch\n",
        "from gpytorch.constraints import GreaterThan, Positive, LessThan\n",
        "\n",
        "#suppress trivial warning from gpytorch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"CG terminated in 1000 iterations\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import trange\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import RecordVideo\n",
        "import base64\n",
        "\n",
        "# IO\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Bu_Pqop0E7"
      },
      "source": [
        "Here is a simple helper function for visualization of episodes directly in this Colab notebook:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so7yH4ucyB-3"
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_videos(path=\"videos\"):\n",
        "    html = []\n",
        "    for mp4 in Path(path).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay\n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFtBY6JSqPFa"
      },
      "source": [
        "Here we set up the parking environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKZt9Cb1rJ6n"
      },
      "source": [
        "env = gym.make(\"parking-v0\") #instantiate the parking environment\n",
        "VIDEO_PATH = './video' #designate the video path\n",
        "env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda e: True, new_step_api=True )\n",
        "env.unwrapped.set_record_video_wrapper(env) #set up virtual monitor to store the video files at VIDEO_PATH dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIjh7uN04ch8"
      },
      "source": [
        "To see the configuration, run the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byyObpA14Dc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ea4373-030d-42ff-e97d-8a8cd44ea59e"
      },
      "source": [
        "env.unwrapped.config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action': {'type': 'ContinuousAction'},\n",
              " 'centering_position': [0.5, 0.5],\n",
              " 'collision_reward': -5,\n",
              " 'controlled_vehicles': 1,\n",
              " 'duration': 100,\n",
              " 'manual_control': False,\n",
              " 'observation': {'features': ['x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'],\n",
              "  'normalize': False,\n",
              "  'scales': [100, 100, 5, 5, 1, 1],\n",
              "  'type': 'KinematicsGoal'},\n",
              " 'offscreen_rendering': False,\n",
              " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
              " 'policy_frequency': 5,\n",
              " 'real_time_rendering': False,\n",
              " 'render_agent': True,\n",
              " 'reward_weights': [1, 0.3, 0, 0, 0.02, 0.02],\n",
              " 'scaling': 7,\n",
              " 'screen_height': 300,\n",
              " 'screen_width': 600,\n",
              " 'show_trajectories': False,\n",
              " 'simulation_frequency': 15,\n",
              " 'steering_range': 0.7853981633974483,\n",
              " 'success_goal_reward': 0.12}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AujR0dDk0g7r"
      },
      "source": [
        "## 1.1. Running an episode with random actions\n",
        "\n",
        "The environment is a `GoalEnv` type, which means the agent receives a dictionary containing both the current `observation` and the `desired_goal` that conditions its policy. In this case, `observation` or `achieved_goal` is the current states and `desired_goal` are the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl6FK9egzX4F"
      },
      "source": [
        "#run an episode\n",
        "env.reset() #resetting the environment to start from a randomized state\n",
        "done = False\n",
        "\n",
        "rewards = []\n",
        "states = []\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample() #use random actions\n",
        "    obs, reward, done, _, info = ...    # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "    #store the state and reward in a list\n",
        "    rewards.append(reward) #store the reward in a list\n",
        "    ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "env.close()\n",
        "\n",
        "#Which variable does store the achieved_goal or observation (s_n) and the desired_goal (s^*)?\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKiWG3bV1hnf"
      },
      "source": [
        "## 1.2. Reward definition\n",
        "\n",
        "The reward (or more specifically penalty, as it uses negative sign) here is the difference between the current state to the targe (parking space)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vgqUe2vzi_h"
      },
      "source": [
        "#Negative reward\n",
        "states_labels = env.config['observation']['features'] #the labels for the states\n",
        "states_scales = env.config['observation']['scales'] #the scales for the states (if needed for plotting)\n",
        "\n",
        "weights = np.array([1, 0.3, 0, 0, 0.02, 0.02]) #weights\n",
        "weighted_norm = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "#plot the weighted norm over time\n",
        "# REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7fgHmrfZOOV"
      },
      "source": [
        "#not needed, but as hinted:\n",
        "#might be useful to compare the calculated weighted norm with the actual reward\n",
        "#they should be the same if your calculation is correct\n",
        "\n",
        "#Reward comparison\n",
        "plt.plot(weighted_norm[:-1], lw=6, label=\"manually calculated\")\n",
        "plt.plot(rewards[:-1], lw=2, label=\"reward from the environment\")\n",
        "plt.title('Reward comparison')\n",
        "plt.xlabel('Timestep')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAH0JQDi2doG"
      },
      "source": [
        "##1.3 Final reward and vehicle trajectory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWbqoBjs3v6-"
      },
      "source": [
        "# final reward\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "# Plot the trajectory of the vehicle (x and y position over timestep)\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvPugEy109pb"
      },
      "source": [
        "#load the video of the episode\n",
        "show_videos('./videos')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIkHaqGd6qNG"
      },
      "source": [
        "### 2.1 Build a dynamics model with NN\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reinitialize the env without recordvideo\n",
        "env = gym.make(\"parking-v0\")"
      ],
      "metadata": {
        "id": "0M-zVcBNAeZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRt5zS-t6o2h"
      },
      "source": [
        "#NN structure\n",
        "class NN_DynamicsModel(nn.Module):\n",
        "    name = 'NN'\n",
        "    def __init__(self, state_size, action_size, hidden_size, dt):\n",
        "        super().__init__()\n",
        "        self.state_size, self.action_size, self.dt = state_size, action_size, dt\n",
        "        self.layer1 = nn.Linear(state_size + action_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, state_size)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        \"\"\"\n",
        "            Predict x_{t+1} = f(x_t, u_t)\n",
        "        :param x: a batch of states\n",
        "        :param u: a batch of actions\n",
        "        :return x_{t+1}\n",
        "        \"\"\"\n",
        "        xu = torch.cat((x, u), -1)\n",
        "        xu = F.relu(self.layer1(xu))\n",
        "        dx = self.layer2(xu).squeeze()\n",
        "\n",
        "        # note that our prediction is delta x: dx = x_{t+1} - x_{t},\n",
        "        # so how to obtain x_{t+1}?\n",
        "\n",
        "        x_next = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "        return x_next"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU_-UHbAa5no"
      },
      "source": [
        "Build NN dynamics model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl-SR6Oea3Yr"
      },
      "source": [
        "#parameters to instantiate the NN model\n",
        "state_size = env.observation_space.spaces[\"observation\"].shape[0] #observation dimension\n",
        "action_size = env.action_space.shape[0] #action dimension\n",
        "hidden_size = 128 #width of the hidden layer\n",
        "timestep_duration = 1/env.unwrapped.config[\"policy_frequency\"] #timestep duration of the environment\n",
        "\n",
        "#instatiate the NN model\n",
        "dynamics_nn = ...  # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXk9J83LhOFX"
      },
      "source": [
        "#run a full episode with random actions and store the untrained nn predictions\n",
        "state_data = []\n",
        "pred_data = []\n",
        "reward_data = []\n",
        "\n",
        "#initialize the environment\n",
        "env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = ...  # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "    state = ...  # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "    pred = dynamics_nn(torch.Tensor(state), torch.Tensor(action)).detach().numpy()\n",
        "\n",
        "    #store the data\n",
        "    ...  # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgXFaE80hrIV"
      },
      "source": [
        "# Plot the true and predicted states\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2PuVAvyfvib",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 2.2 Model training\n",
        "\n",
        "First, we randomly interact with the environment to produce a batch of experiences.\n",
        "$$D = \\{s_t, a_t, s_{t+1}\\}_{t\\in[1,N]}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvUYSL7sfvie",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "#functions to create experiences dataset D\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'next_state'])\n",
        "\n",
        "def collect_interaction_data(env, size=2000, action_repeat=1):\n",
        "    data, done = [], True\n",
        "    for _ in trange(size):\n",
        "        action = env.action_space.sample() #random sample actions\n",
        "        for _ in range(action_repeat):\n",
        "            previous_obs = env.reset() if done else obs\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            data.append(Transition(torch.Tensor(previous_obs[\"observation\"]),\n",
        "                                   torch.Tensor(action),\n",
        "                                   torch.Tensor(obs[\"observation\"])))\n",
        "    return data\n",
        "\n",
        "def transpose_batch(batch):\n",
        "    return Transition(*map(torch.stack, zip(*batch)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b39DfwWJrSKd"
      },
      "source": [
        "size = ...  # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "data = collect_interaction_data(env, size=size)\n",
        "\n",
        "print(\"Sample transition:\", data[0])\n",
        "print(\"Buffer size:\", len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwCDLD1wfvi2",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "# Split dataset into training and validation\n",
        "\n",
        "train_ratio = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "train_data, validation_data = data[:int(train_ratio * len(data))], \\\n",
        "                              data[int(train_ratio * len(data)):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4W_BruVsD-Z"
      },
      "source": [
        "# functions to train the model\n",
        "\n",
        "def compute_loss(model, data_t, loss_func = torch.nn.MSELoss()):\n",
        "    states, actions, next_states = data_t\n",
        "    predictions = model(states, actions)\n",
        "    return loss_func(predictions, next_states)\n",
        "\n",
        "def train(model, train_data, validation_data, epochs=1500, learning_rate = 0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_data_t = transpose_batch(train_data)\n",
        "    validation_data_t = transpose_batch(validation_data)\n",
        "    losses = np.full((epochs, 2), np.nan)\n",
        "\n",
        "    for epoch in trange(epochs):\n",
        "        # Compute loss gradient and step optimizer\n",
        "        loss = compute_loss(model, train_data_t)\n",
        "        validation_loss = compute_loss(model, validation_data_t)\n",
        "        losses[epoch] = [loss.detach().numpy(), validation_loss.detach().numpy()]\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Plot losses\n",
        "    plt.plot(losses)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend([\"training\", \"validation\"])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxNFrBccbF9T"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npTKFvkJ1RKt"
      },
      "source": [
        "num_epochs = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "learning_rate = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "train(dynamics_nn, train_data, validation_data, epochs=num_epochs, learning_rate = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izS9PQzl6KKg"
      },
      "source": [
        "## 2.3 Predictions of the trained NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE8uwGB-jOX3"
      },
      "source": [
        "dynamics_nn.eval() #set the trained model for use\n",
        "\n",
        "#run a full episode with random actions and store the trained nn predictions\n",
        "state_data = []\n",
        "pred_data = []\n",
        "reward_data = []\n",
        "\n",
        "#initialize the environment\n",
        "env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    state = ...  # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "    pred = dynamics_nn(torch.Tensor(state), torch.Tensor(action)).detach().numpy()\n",
        "\n",
        "    #store the data\n",
        "    ...  # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3wLJdo5vnTB"
      },
      "source": [
        "# Plot the true and predicted states\n",
        "states = np.array(state_data)\n",
        "preds = np.array(pred_data)\n",
        "\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "nn_error = ((states-preds)**2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXBODCuYfvi_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## CEM planner\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRubRv9buNXj"
      },
      "source": [
        "#functions to help build the  CEM method\n",
        "def predict_trajectory_actionlist(state, actions, model, action_repeat=1):\n",
        "    states = []\n",
        "    for action in actions:\n",
        "        for _ in range(action_repeat):\n",
        "            if model.name == 'GP':\n",
        "                state = model(torch.cat((state, action), -1)).mean\n",
        "            else:\n",
        "                state = model(state, action)\n",
        "            states.append(state)\n",
        "    return torch.stack(states, dim=0)\n",
        "\n",
        "def reward_model(states, goal, gamma=None):\n",
        "    \"\"\"\n",
        "        The reward is a weighted L1-norm between the state and a goal\n",
        "    :param Tensor states: a batch of states. shape: [batch_size, state_size].\n",
        "    :param Tensor goal: a goal state. shape: [state_size].\n",
        "    :param float gamma: a discount factor\n",
        "    \"\"\"\n",
        "    goal = goal.expand(states.shape)\n",
        "    reward_weigths = torch.Tensor(env.unwrapped.config[\"reward_weights\"])\n",
        "    rewards = -torch.pow(torch.norm((states-goal)*reward_weigths, p=1, dim=-1), 0.5)\n",
        "    if gamma:\n",
        "        time = torch.arange(rewards.shape[0], dtype=torch.float).unsqueeze(-1).expand(rewards.shape)\n",
        "        rewards *= torch.pow(gamma, time)\n",
        "    return rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5D6W4p7fvjI"
      },
      "source": [
        "We now use the learnt dynamics model $f_\\theta$ for planning.\n",
        "In order to solve the optimal control problem, we use a sampling-based optimization algorithm: the **Cross-Entropy Method** (`CEM`). It is an optimization algorithm applicable to problems that are both **combinatorial** and **continuous**, which is our case: find the best performing sequence of actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzPKYg23fvjL",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "def cem_planner(state, goal, action_size, model, horizon=5, population=10, selection=5, iterations=5):\n",
        "    state = state.expand(population, -1)\n",
        "    action_mean = torch.zeros(horizon, 1, action_size)\n",
        "    action_std = torch.ones(horizon, 1, action_size)\n",
        "\n",
        "    for iter_num in range(iterations):\n",
        "        # 1. Draw sample sequences of actions from a normal distribution\n",
        "        actions = action_mean + action_std * torch.randn(horizon, population, action_size)\n",
        "        actions = torch.clamp(actions, min=env.action_space.low.min(), max=env.action_space.high.max())\n",
        "        states = predict_trajectory_actionlist(state, actions, model, action_repeat=horizon)\n",
        "\n",
        "        # 2. Fit the distribution to the top-k performing sequences\n",
        "        returns = reward_model(states, goal).sum(dim=0)\n",
        "        _, best = returns.topk(selection, largest=True, sorted=False)\n",
        "        best_actions = actions[:, best, :]\n",
        "\n",
        "        action_std = best_actions.std(dim=1, unbiased=False, keepdim=True)\n",
        "        action_mean = ...  # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "    return action_mean[0].squeeze(dim=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKK-WkDh0z_Q"
      },
      "source": [
        "Example of using the CEM to select actions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBxoEeqv7DMn"
      },
      "source": [
        "# Run the planner on a sample transition\n",
        "\n",
        "obs = env.reset() #reset the environment to start\n",
        "\n",
        "#run CEM to get actions using NN dynamics model\n",
        "H = 3 #planning horizon\n",
        "K = 10 #population size\n",
        "\n",
        "action_nn = cem_planner(torch.Tensor(obs[\"observation\"]),\n",
        "                     torch.Tensor(obs[\"desired_goal\"]),\n",
        "                     env.action_space.shape[0], dynamics_nn,\n",
        "                     horizon=H, population=K)\n",
        "\n",
        "print(\"Planned action:\", action_nn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sw6vxTu0-pL"
      },
      "source": [
        "## 2.4. CEM with NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1UXtD3fuwla"
      },
      "source": [
        "#run an episode for nn\n",
        "\n",
        "#run an episode with random actions for at most n time steps\n",
        "n = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "H = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "K = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "import time\n",
        "\n",
        "nn_time = []\n",
        "state_data_nn = []\n",
        "reward_data_nn = []\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "i=0 #iterates\n",
        "\n",
        "while (not done and i < n) :\n",
        "\n",
        "    #use CEM with NN model to select actions\n",
        "    start = time.time()\n",
        "    state = obs[\"observation\"]\n",
        "    target = obs[\"desired_goal\"]\n",
        "    # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "    action = ...\n",
        "    obs, reward, done, info = ...\n",
        "\n",
        "    end = time.time()\n",
        "    #store the data\n",
        "    ...\n",
        "\n",
        "    i+=1\n",
        "    print(i, end=\"\\n\")\n",
        "\n",
        "env.close()\n",
        "nn_reward = reward_data_nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8tAbKYR_MAV"
      },
      "source": [
        "# Plot the rewards over time step\n",
        "\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDcRWzl36xO7"
      },
      "source": [
        "### 3.1 Build a dynamics model with a Gaussian process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lMVTo1KaGM3"
      },
      "source": [
        "#GP model structure\n",
        "#More details are available in gpytorch documentation: https://docs.gpytorch.ai/en/stable/\n",
        "\n",
        "import gpytorch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
        "    name = 'GP'\n",
        "    def __init__(self, train_x, train_y, likelihood, num_tasks=6): # number tasks is equivalent to the output dim\n",
        "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.num_tasks = num_tasks\n",
        "        self.mean_module = gpytorch.means.MultitaskMean(\n",
        "            gpytorch.means.ConstantMean(), num_tasks=num_tasks\n",
        "        )\n",
        "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
        "            gpytorch.kernels.RBFKernel(), num_tasks=num_tasks, rank=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        dists = gpytorch.distributions.MultitaskMultivariateNormal(mean_x + x[:,:self.num_tasks], covar_x)\n",
        "        return dists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su_vEjfjaOPU"
      },
      "source": [
        "Build data pre-processing and prediction functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwTCXvwpaKkk"
      },
      "source": [
        "#functions to prepare the data for GP training\n",
        "\n",
        "def GP_dataloader_process(data, train_ratio = 0.3):\n",
        "    # Split dataset into training and validation\n",
        "    data_n = transpose_batch(data)\n",
        "    inputs = torch.cat((data_n[0], data_n[1]), dim=1)\n",
        "    targets = data_n[2]\n",
        "    data_for_loader = []\n",
        "\n",
        "    for i in range(inputs.shape[0]):\n",
        "      data_for_loader.append([inputs[i], targets[i]])\n",
        "\n",
        "    # prepare data for loader\n",
        "    train_data_for_loader, validation_data_for_loader = data_for_loader[:int(train_ratio * len(data))], data_for_loader[int(train_ratio * len(data)):]\n",
        "\n",
        "    # prepare data for GP\n",
        "    return train_data_for_loader, validation_data_for_loader\n",
        "\n",
        "def GP_data_prepare(data, train_ratio = 0.3):\n",
        "    # Split dataset into training and validation\n",
        "    train_data, validation_data = data[:int(train_ratio * len(data))], \\\n",
        "                                  data[int(train_ratio * len(data)):]\n",
        "    train_data_t = transpose_batch(train_data)\n",
        "    states, actions, next_states = train_data_t\n",
        "    train_x = torch.cat((states, actions), -1)\n",
        "    train_y = next_states\n",
        "\n",
        "    validation_data_t = transpose_batch(validation_data)\n",
        "    val_states, val_actions, val_next_states = validation_data_t\n",
        "    val_x = torch.cat((val_states, val_actions), -1)\n",
        "    val_y = val_next_states\n",
        "    return [train_x, train_y], [val_x, val_y]\n",
        "\n",
        "def predict_trajectory(state, action, model, action_repeat=1):\n",
        "    states = []\n",
        "    for _ in range(action_repeat):\n",
        "        if model.name == 'GP':\n",
        "            state = model(torch.cat((state, action), -1)).mean\n",
        "        else:\n",
        "            state = model(state, action)\n",
        "        states.append(state)\n",
        "    return torch.stack(states, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoDu4HfhaXXZ"
      },
      "source": [
        "Training the GP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhusHG3d61zU"
      },
      "source": [
        "train_ratio = 0.2\n",
        "\n",
        "#prepare and split the dataset\n",
        "train_data, validation_data = GP_data_prepare(data, train_ratio=train_ratio)\n",
        "train_x, train_y = train_data\n",
        "val_x, val_y = validation_data\n",
        "\n",
        "# construct dynamics model\n",
        "output_dim = 6 #the dimension of the state variables\n",
        "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=output_dim, noise_constraint=GreaterThan(0.000001))\n",
        "\n",
        "#instantiate the model\n",
        "dynamics_gp = MultitaskGPModel(train_x, train_y, likelihood, num_tasks=output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VXBPy4v6shp"
      },
      "source": [
        "learning_rate = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "epochs = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, dynamics_gp) #loss function\n",
        "optimizer = torch.optim.Adam([{'params': dynamics_gp.parameters()},], lr=learning_rate) #adam optimizer\n",
        "\n",
        "losses = np.full((epochs, 2), np.nan) #instantiate variables to store loss values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHIQ7WTRxqa2"
      },
      "source": [
        "\n",
        "#iterate the training\n",
        "for i in trange(epochs):\n",
        "    dynamics_gp.train()\n",
        "    likelihood.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = dynamics_gp(train_x)\n",
        "    loss = -mll(output, train_y) #compute the loss\n",
        "    loss.backward() #backprop the error\n",
        "    optimizer.step() #update the nn params\n",
        "\n",
        "    dynamics_gp.eval()\n",
        "    likelihood.eval()\n",
        "    val_out = dynamics_gp(val_x)\n",
        "    validation_loss = -mll(val_out, val_y) #compute the validation loss\n",
        "\n",
        "    losses[i] = [loss.detach().numpy(), validation_loss.detach().numpy()] #store the computed loss\n",
        "\n",
        "    print('Iter %d/%d - Loss: %.3f - Val Los: %.3f' % (i + 1, epochs, loss.item(), validation_loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvZjRAIF8HSt"
      },
      "source": [
        "# Plot the training and validation losses\n",
        "\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaCGA-2ox39i"
      },
      "source": [
        "##3.2 Using the trained model for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-zQZdM7Epd"
      },
      "source": [
        "#run an episode with random actions for at most n time steps\n",
        "n = 20\n",
        "\n",
        "state_data_gp = []\n",
        "pred_data_gp = []\n",
        "reward_data_gp = []\n",
        "\n",
        "env.reset()\n",
        "done = False\n",
        "i=0 #iterates\n",
        "\n",
        "while (not done and i < n) :\n",
        "\n",
        "    # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "    action = ...\n",
        "    obs, reward, done, info = ...\n",
        "\n",
        "    state = torch.Tensor(obs['observation']).unsqueeze(0) #convert the state to proper format\n",
        "    action = torch.Tensor(action).unsqueeze(0)  #convert the action to proper format\n",
        "\n",
        "    pred = predict_trajectory(state, action, dynamics_gp, action_repeat=1).detach().numpy() #GP prediction\n",
        "\n",
        "    #store the data\n",
        "    # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "    ...\n",
        "\n",
        "    i+=1\n",
        "    print(i, end=\"\\n\")\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI5aeJkLuwlY"
      },
      "source": [
        "# Plot the true and predicted states\n",
        "\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrMRIOTz2L5q"
      },
      "source": [
        "## 3.3. CEM with GP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W64w5kpP_B2P"
      },
      "source": [
        "#run an episode for GP\n",
        "\n",
        "#run an episode with random actions for at most n time steps\n",
        "n = ... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "state_data = []\n",
        "reward_data = []\n",
        "gp_time = []\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "i=0 #iterates\n",
        "state = torch.Tensor(obs['observation']).unsqueeze(0)\n",
        "target = torch.Tensor(obs['desired_goal']).unsqueeze(0)\n",
        "\n",
        "while (not done and i < n) :\n",
        "\n",
        "    start = time.time()\n",
        "    # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "    action = ... #use CEM with GP model to select actions\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    state = torch.Tensor(obs['observation']).unsqueeze(0) #convert the state to proper format\n",
        "    action = torch.Tensor(action).unsqueeze(0)  #convert the action to proper format\n",
        "\n",
        "    pred = predict_trajectory(state, action, dynamics_gp, action_repeat=1).detach().numpy() #GP prediction\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    #store the data\n",
        "    state_data.append(np.array(state).reshape(-1,))\n",
        "    reward_data.append(reward)\n",
        "    gp_time.append(end-start)\n",
        "\n",
        "    i+=1\n",
        "    print(i, end=\"\\n\")\n",
        "\n",
        "env.close()\n",
        "gp_reward = reward_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej70_JGV_JaZ"
      },
      "source": [
        "# Plot the rewards over time step\n",
        "\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE9MpgdAIAKx"
      },
      "source": [
        "# Get the final reward (when using NN and GP with CEM)\n",
        "\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AOGRk3dH-0w"
      },
      "source": [
        "Computing the prediction error, and computation time between NN and GP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38tmfO_cIGzJ"
      },
      "source": [
        "# Compute the prediction error of NN and GP models during the planning horizon\n",
        "\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE\n",
        "\n",
        "\n",
        "# Get the computation time of running CEM with NN (from 2.4)and GP (from 3.3)\n",
        "... # REPLACE THE THREE DOTS WITH YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8L6vEPWyea7"
      },
      "source": [
        "##Visualize a few episodes [Optional]\n",
        "\n",
        "Here is a code to visualize a few episodes with CEM action planner.  This is optional, but might be helpful to gain insights.\n",
        "\n",
        "En voiture, Simone!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOcOP7Of18T2"
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "env = gym.make(\"parking-v0\") #instantiate the parking environment\n",
        "VIDEO_PATH = './video-final' #designate the video path\n",
        "env = RecordVideo(env, video_folder=VIDEO_PATH, episode_trigger=lambda e: True, new_step_api=True)\n",
        "env.unwrapped.set_record_video_wrapper(env) #set up virtual monitor to store the video files at VIDEO_PATH dir\n",
        "num_episodes = 5\n",
        "H = 6\n",
        "K = 25\n",
        "iterations = 15\n",
        "\n",
        "#select the dynamic model\n",
        "selected_dynamic_model = dynamics_nn #we will use NN model for its fast computation\n",
        "\n",
        "for episode in trange(num_episodes):\n",
        "    obs = env.reset()\n",
        "    done=False\n",
        "    while (not done):\n",
        "        action = cem_planner(torch.Tensor(obs[\"observation\"]),\n",
        "                             torch.Tensor(obs[\"desired_goal\"]),\n",
        "                             env.action_space.shape[0], selected_dynamic_model,\n",
        "                             horizon=H, population=K, iterations=iterations)\n",
        "        obs, reward, done, _, info = env.step(action.numpy())\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEu3RG37uwlb"
      },
      "source": [
        "show_videos(VIDEO_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}